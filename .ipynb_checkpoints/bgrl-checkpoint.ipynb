{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitive RL in the basal ganglia\n",
    "\n",
    "This is a notebook to explore the idea that the basal ganglia implement a competitive RL system that gives rise to unbiased estimates of state-action values $Q(s,a)$ over short amounts of time.\n",
    "\n",
    "The main idea is that the basal ganglia relies on two opposing RL systems, a _positive RL agent_  implemented by the direct pathway and a _negative RL agent_ implemented by the indirect pathway. The direct pathway assigns values for actions to take; the indirect pathway assigns values for actions to avoid. The postive RL agent is a standard RL agent. Under certain circumstances, this competitive system provides a quicker and most efficient learning that a standard RL system.\n",
    "\n",
    "## Defining the task\n",
    "\n",
    "The problems with canonical RL systems are well illustrated by the Probabilistic Stimulus Selection task, first introduced by Frank, Seeberger, and O'Reilly (2004). The PSS task is implemented here. In the PSS task, the set of possible actions $\\mathbf{A}$ includes only six possible actions $\\mathbf{A} = \\{A, B, C, D, E, F\\}$. An _action_ is such if and only if it belongs to $\\mathbf{A}$. Unbeknowst to the agent, each action yields a probabilistic reward with a characteristic probability, and the probability of obtaining a reward depends only the action taken $a_t$, and not on the previous state $s_t$, i.e. $P(r_{t+1} > 0 \\mid s_{t}, a_{t}) = P(r_{t+1} > 0 \\mid a_t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PSS_Object():\n",
    "    \"\"\"A generic class for PSS objects\"\"\"\n",
    "    ACTIONS = (\"A\", \"C\", \"E\", \"F\", \"D\", \"B\")\n",
    "    \n",
    "    REWARD_TABLE = {\"A\" : 0.8, \"C\" : 0.7, \"E\" : 0.6,\n",
    "                    \"F\" : 0.4, \"D\" : 0.3, \"B\" : 0.2}\n",
    "\n",
    "    def is_action(self, action):\n",
    "        \"\"\"An action is valid only if it belongs to the list of possible actions\"\"\"\n",
    "        return action in self.ACTIONS\n",
    "    \n",
    "    def prob_reward(self, action):\n",
    "        \"\"\"Returns the probability of obtaining a reward given an action\"\"\"\n",
    "        if self.is_action(action):\n",
    "            return self.REWARD_TABLE[action]\n",
    "        \n",
    "    def get_reward(self, action):\n",
    "        \"\"\"Return a probabilistic reward associated with an action\"\"\"\n",
    "        i = random.random()\n",
    "        if i <= self.prob_reward(action):\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "        \n",
    "    \n",
    "o = PSS_Object()\n",
    "o.is_action(\"A\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The State class\n",
    "\n",
    "In the PSS task, a state $s$ is defined as the presentation of two options, $o_1$ and $o_2$: $s = (o_1, o_2)$ We encapsulate this concept into a new class `PSS_State`, which contains the available options as a tuple. Two options in a state cannot be identical, i.e. $o_1 \\neq o_2$.  Two states $s_j$ and $s_j$ are equal if their options are equal, irrespective of order; that is $s_i = s_j \\Leftrightarrow (o_{1}^{i}, o_{2}^{i}) = (o_{1}^{j}, o_{2}^{j}) \\lor (o_{1}^{j}, o_{2}^{j}) = (o_{2}^{j}, o_{1}^{j})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PSS_State(PSS_Object):\n",
    "    \"\"\"A state in the PSS object\"\"\"\n",
    "    def __init__(self, options = (\"A\", \"B\")):\n",
    "        if self.is_options(options):\n",
    "            self.options = options\n",
    "            self.left = options[0]\n",
    "            self.right = options[1]\n",
    "        else:\n",
    "            self.options = None\n",
    "\n",
    "    def is_options(self, options):\n",
    "        \"\"\"Checks whether a given tuple is a set of options\"\"\"\n",
    "        if len(options) == 2 and not False in [x in self.ACTIONS for x in options]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Equality if the options are the same, independent of order\"\"\"\n",
    "        return (self.left == other.left and self.right == other.right) or \\\n",
    "               (self.left == other.right and self.right == other.left)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"(%s,%s)\" % (self.left, self.right)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "s1 = PSS_State((\"A\", \"B\"))\n",
    "s2 = PSS_State((\"B\", \"A\"))\n",
    "s1 == s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decision class\n",
    "\n",
    "We also define a class `PSS_Decision`, every instance $d$ of which is a combination of a state $s_t$, the action $a_t$ that an agent has taken in that state, and the reward $r_{t+1}$ that the agent has received as a consequence of its action: $d = (s_t, a_t, r_{t+1})$. Instances of this class will be used for collecting measures of an agent's performance. A decision is considered _successful_ if $r_{t+1} > 0$, and _optimal_ if $a_t$ is the action that is associated to the greater probability of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PSS_Decision(PSS_Object):\n",
    "    \"\"\"A decision made during the PSS task\"\"\"\n",
    "    def __init__(self, state = None, action = None, reward = 0.0):\n",
    "        if self.is_state(state) and self.is_action(action) and action in state.options:\n",
    "            self.state = state\n",
    "            self.action = action\n",
    "            self.reward = 0.0\n",
    "    \n",
    "    def is_state(self, state):\n",
    "        return True\n",
    "    \n",
    "    @property\n",
    "    def successful(self):\n",
    "        \"\"\"Success is reward > 0.\"\"\"\n",
    "        if self.reward > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    @property\n",
    "    def optimal(self):\n",
    "        \"\"\"Determines if an action was optimal\"\"\"\n",
    "        s = self.state\n",
    "        apos = s.options.index(self.action)\n",
    "        probs = [self.prob_reward(x) for x in s.options]\n",
    "        ppos = probs.index(max(probs))\n",
    "        return apos == ppos\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"The decision as a string\"\"\"\n",
    "        return \"<%s, %s, %0.1f>\" % (self.state, self.action, self.reward)\n",
    "\n",
    "d = PSS_Decision(PSS_State(), \"A\", 1.0)\n",
    "d.optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PSS Task\n",
    "\n",
    "The PSS task is a repetitive, two-alternative forced-choice task. The task is made of two consecutive phases, a _training_ phase where the agent makes repetive choices between fixed pairs of alternatives and learns the value of every action, and a _test_ phase where the agent faces new combinations of options. In human experiments, a third phase, _practice_ occurs before training to ensure that participants do understand the task; this phase is obviously not needed in a model.\n",
    "\n",
    "Participants proceed through one or more training blacks, until they have reached a predefined criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test': deque([(E,A),\n",
       "        (E,B),\n",
       "        (B,C),\n",
       "        (C,B),\n",
       "        (B,C),\n",
       "        (B,E),\n",
       "        (C,B),\n",
       "        (B,D),\n",
       "        (E,B),\n",
       "        (A,E),\n",
       "        (A,D),\n",
       "        (B,E),\n",
       "        (A,E),\n",
       "        (D,B),\n",
       "        (C,A),\n",
       "        (E,A),\n",
       "        (A,C),\n",
       "        (C,A),\n",
       "        (A,C),\n",
       "        (D,A),\n",
       "        (B,D),\n",
       "        (D,A),\n",
       "        (D,B),\n",
       "        (A,D)]),\n",
       " 'Training': deque([(A,B),\n",
       "        (C,D),\n",
       "        (B,A),\n",
       "        (A,B),\n",
       "        (A,B),\n",
       "        (E,F),\n",
       "        (C,D),\n",
       "        (B,A),\n",
       "        (E,F),\n",
       "        (B,A),\n",
       "        (E,F),\n",
       "        (E,F),\n",
       "        (B,A),\n",
       "        (C,D),\n",
       "        (D,C),\n",
       "        (C,D),\n",
       "        (E,F),\n",
       "        (D,C),\n",
       "        (C,D),\n",
       "        (C,D),\n",
       "        (D,C),\n",
       "        (A,B),\n",
       "        (A,B),\n",
       "        (E,F),\n",
       "        (D,C),\n",
       "        (E,F),\n",
       "        (B,A),\n",
       "        (B,A),\n",
       "        (A,B),\n",
       "        (E,F),\n",
       "        (A,B),\n",
       "        (D,C),\n",
       "        (C,D),\n",
       "        (D,C),\n",
       "        (C,D),\n",
       "        (F,E),\n",
       "        (B,A),\n",
       "        (C,D),\n",
       "        (E,F),\n",
       "        (B,A),\n",
       "        (D,C),\n",
       "        (B,A),\n",
       "        (D,C),\n",
       "        (A,B),\n",
       "        (E,F),\n",
       "        (A,B),\n",
       "        (A,B),\n",
       "        (B,A),\n",
       "        (C,D),\n",
       "        (D,C),\n",
       "        (D,C)])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "class PSS_Task(PSS_Object):\n",
    "    \"\"\"An object implementing the PSS task\"\"\"\n",
    "    TRAINING_BLOCK = (((\"A\", \"B\"),) * 10 +\n",
    "                      ((\"B\", \"A\"),) * 10 +\n",
    "                      ((\"C\", \"D\"),) * 10 +\n",
    "                      ((\"D\", \"C\"),) * 10 +\n",
    "                      ((\"E\", \"F\"),) * 10 +\n",
    "                      ((\"F\", \"E\"),))\n",
    "    \n",
    "    TEST_BLOCK = (((\"A\", \"C\"),) * 2 + ((\"C\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"D\"),) * 2 + ((\"D\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"E\"),) * 2 + ((\"E\", \"A\"),) * 2 +\n",
    "                  ((\"B\", \"C\"),) * 2 + ((\"C\", \"B\"),) * 2 +\n",
    "                  ((\"B\", \"D\"),) * 2 + ((\"D\", \"B\"),) * 2 +\n",
    "                  ((\"B\", \"E\"),) * 2 + ((\"E\", \"B\"),) * 2)\n",
    "    \n",
    "    PHASES = (\"Training\", \"Test\")\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PSS task experiment\"\"\"\n",
    "        self.index = 0\n",
    "        self.state = None\n",
    "        self.phase = \"Training\"\n",
    "        \n",
    "        self.train = self.instantiate_block(self.TRAINING_BLOCK)        \n",
    "        self.test =  self.instantiate_block(self.TEST_BLOCK)\n",
    "        self.blocks = dict(zip(self.PHASES, [self.train, self.test]))                \n",
    "        self.history = dict(zip(self.PHASES, [[], []]))\n",
    "\n",
    "    \n",
    "    def instantiate_block(self, block):\n",
    "        \"\"\"Instantiates and randomizes a block of trials\"\"\"\n",
    "        trials = [PSS_State(x) for x in block]\n",
    "        random.shuffle(trials)\n",
    "        return deque(trials)\n",
    "    \n",
    "    def criterion_reached(self):\n",
    "        \"\"\"Reached criterion for successful learning\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def next_state(self):\n",
    "        \"\"\"Next state (transitions are independent of actions)\"\"\"\n",
    "        state_next = None\n",
    "        current_block = self.blocks[self.phase]\n",
    "        if len(current_block) == 0:\n",
    "            if self.phase == \"Training\":\n",
    "                if self.criterion_reached():\n",
    "                    # Move to the Test phase\n",
    "                    self.phase = \"Test\"\n",
    "                else:\n",
    "                    self.blocks[\"Training\"] = self.instantiate_block(self.TRAINING_BLOCK)\n",
    "                \n",
    "                state_next = current_block.popleft()\n",
    "            \n",
    "            else: \n",
    "                state_next = None # End of the experiment\n",
    "        else:\n",
    "            state_next = current_block.popleft()\n",
    "            \n",
    "        return state_next\n",
    "                    \n",
    "    \n",
    "    def execute_action(self, action):\n",
    "        \"\"\"Executes and action and returns the new state and a reward\"\"\"\n",
    "        if self.is_action(action):\n",
    "            r = self.get_reward(action)\n",
    "            \n",
    "            # Update history\n",
    "            d = PSS_Decision(self.state, self.action, self.next_r)\n",
    "            self.history[self.phase].append(d)\n",
    "            \n",
    "            self.state = self.next_state()\n",
    "            return (self.state(), r)\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "p = PSS_Task()\n",
    "p.history\n",
    "p.blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random, copy, collections\n",
    "z = [4, 5, 2, 1]\n",
    "random.shuffle(copy.copy(z))\n",
    "q=collections.deque(z)\n",
    "q\n",
    "q.popleft()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "Here we can define the agents, and how they learn. An agent $A$ perceives the experiments's state, and decides which actions to perform.  Because not all the actions are available to every state, the agent will have to improvise.\n",
    "\n",
    "All agents are instances of the class `PSS_Agent`, and all inherit a simple mechanism to interact with the experiment  \n",
    "\n",
    "## A Q-Learning agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PSS_Agent():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
